# -*- coding: utf-8 -*-
"""Transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KHO4gCU-CjtTNhY0RPsmxkncEkbLaXnR
"""

from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os
import torch
import torch.nn as nn
import unicodedata
import string
import numpy
import re
import pandas as pd
import torch
from torch import nn, Tensor
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder , TransformerDecoderLayer
from torch.utils.data import dataset

import torch.nn.functional as F

import numpy as np
import string
import time 
import nltk
import gensim, logging

from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
#from keras_preprocessing.sequence import pad_sequences
import math
string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'
string.punctuation = string.punctuation.replace('.', '') #why replace the dot with empty shit. Because we want to get senteneces make senseb

#pathlib.PosixPath
from pathlib import Path


vocab_size = 20032
seq_length = 50
d_model = 300


X_file = 'Xdata_npy.npy'
Y_file = 'Ydata_npy.npy'

X = np.load("C:\\University study material\\Project lab\\LSTM on Aristotle\\" + X_file, allow_pickle=True)
Y = np.load("C:\\University study material\\Project lab\\LSTM on Aristotle\\" + Y_file, allow_pickle=True)
 

X_tensor = torch.tensor(X)
Y_tensor = torch.Tensor(Y)
Y_tensor = Y_tensor.to(torch.long)



print(type(X[0]))

class Embedder(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding_file = 'word2vec-google-news-300.model.vectors.npy'
        self.embedding = np.load("C:\\University study material\\Project lab\\LSTM on Aristotle\\" + self.embedding_file)

    def forward(self, x):
        return torch.tensor(self.embedding[x]).unsqueeze(0)




class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -> Tensor:
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)




# Example usage
tgt_seq_len = 10
batch_size = 4
num_heads = 8
nhead = 1
d_hid = 2048
nlayers = 2
BATCH_SIZE = 32

class Norm(nn.Module):
    def __init__(self, d_model, eps = 1e-6):
        super().__init__()
    
        self.size = d_model
        
        # create two learnable parameters to calibrate normalisation
        self.alpha = nn.Parameter(torch.ones(self.size))
        self.bias = nn.Parameter(torch.zeros(self.size))
        
        self.eps = eps
    
    def forward(self, x):
        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
        return norm

def attention(q, k, v, d_k, mask=None, dropout=None):
    
    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)
    
    if mask is not None:
        mask = mask.unsqueeze(1)
        scores = scores.masked_fill(mask == 0, -1e9)
    
    scores = F.softmax(scores, dim=-1)
    
    if dropout is not None:
        scores = dropout(scores)
        
    output = torch.matmul(scores, v)
    return output

class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.d_k = d_model // heads
        self.h = heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)
    
    def forward(self, q, k, v, mask=None):
        
        bs = q.size(0)
        
        # perform linear operation and split into N heads
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)
        
        # transpose to get dimensions bs * N * sl * d_model
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)
        

        # calculate attention using function we will define next
        scores = attention(q, k, v, self.d_k, mask, self.dropout)
        # concatenate heads and put through final linear layer
        concat = scores.transpose(1,2).contiguous()\
        .view(bs, -1, self.d_model)
        output = self.out(concat)
    
        return output

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048, dropout = 0.1):
        super().__init__() 
    
        # We set d_ff as a default to 2048
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)
    
    def forward(self, x):
        x = self.dropout(F.relu(self.linear_1(x)))
        x = self.linear_2(x)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.ff = FeedForward(d_model, dropout=dropout)
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        
    def forward(self, x):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn(x2,x2,x2))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.ff(x2))
        return x

class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, dropout):
        super().__init__()
        self.N = N
        self.pe = PositionalEncoding(d_model, dropout=dropout)
        self.layers = EncoderLayer(d_model, heads, dropout)
        self.norm = Norm(d_model)
    def forward(self, src):
        x = self.pe(src)        
        x = self.layers(x)
        return self.norm(x)


class DecoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.norm_3 = Norm(d_model)
        
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        self.dropout_3 = nn.Dropout(dropout)
        
        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.ff = FeedForward(d_model, dropout=dropout)

    def forward(self, x, e_outputs):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn_1(x2, x2, x2))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs))
        x2 = self.norm_3(x)
        x = x + self.dropout_3(self.ff(x2))
        return x

 
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, dropout):
        super().__init__()
        self.N = N
        self.pe = PositionalEncoding(d_model, dropout=dropout)
        self.layers = DecoderLayer(d_model, heads, dropout)
        self.norm = Norm(d_model)
    def forward(self, trg, e_outputs):
        x = self.pe(trg)
        x = self.layers(x, e_outputs)
        return self.norm(x)

class Transformer(nn.Module):
    def __init__(self, src_vocab, d_model, N, heads, dropout):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)
        self.decoder = Decoder(src_vocab, d_model, N, heads, dropout)
        self.out = nn.Linear(d_model, d_model)
    def forward(self, src):
        e_outputs = self.encoder(src)
        #print("DECODER")
        d_output = self.decoder(src, e_outputs)
        output = self.out(d_output)
        return output

Transformer_model = Transformer(vocab_size, d_model, 1,1,dropout = 0.1)
criterion = nn.MSELoss()
lr = .01  # learning rate
optimizer = torch.optim.Adam(Transformer_model.parameters(), lr=lr)
#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)
Embedder_obj = Embedder()
tot_loss = []
def train() -> None:
    Transformer_model.train()  # turn on train mode
    total_loss = 0
    log_interval = 200
    start_time = time.time()
   # num_batches = len(train_data) // bptt
    for j in range(20):
      print(f"Epoch = {j}")
      print("-----------------------------------------------------------------------------------")
      if (j > 0):
        torch.save(Transformer_model.state_dict(), 'weights/model_weights')
      for i in (range(0, X_tensor.size(0) - 1, BATCH_SIZE+1)):
          X_tensor_e = Embedder_obj(X_tensor[i + BATCH_SIZE ])
          Y_tensor_e = Embedder_obj(Y_tensor[i + BATCH_SIZE])
          Y_tensor_e = Y_tensor_e.to(torch.float)

          output = Transformer_model(X_tensor_e) 
         # output = output.to(torch.long)
          loss = criterion(output,  Y_tensor_e)
          if(i == 0):
            tot_loss.append(loss)
          #if(i %50 ==0):
         
          if (i == 0):
            print(f" epoch = {j} and loss = {loss}")

          if (i%50 == 0):
            print(f" inside epoch = {j} here is an update on loss = {loss}")
          optimizer.zero_grad()
          loss.backward()
          torch.nn.utils.clip_grad_norm_(Transformer_model.parameters(), 0.5)
          optimizer.step()
          total_loss += loss.item()

    with open('all losses file', 'w') as f:  
            # using csv.writer method from CSV package
        write = csv.writer(f)  
        write.writerow(tot_loss)



train()

